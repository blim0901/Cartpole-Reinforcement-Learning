{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"UBiYOoesYMvr"},"source":["## Installing dependencies:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71445,"status":"ok","timestamp":1680101252362,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"PbgnVwZmX5uW","outputId":"ea65b7ba-7738-4a9d-83c4-4a9bb9cc6bfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (6.1.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.2.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (1.22.4)\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n","Installing collected packages: pygame\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.0)\n","Collecting setuptools\n","  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: setuptools\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.6.0\n","    Uninstalling setuptools-67.6.0:\n","      Successfully uninstalled setuptools-67.6.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed setuptools-67.6.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyglet\n","  Downloading pyglet-2.0.5-py3-none-any.whl (831 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.3/831.3 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyglet\n","Successfully installed pyglet-2.0.5\n"]}],"source":["\n","!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install gym[classic_control]\n","!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install pyglet"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44174,"status":"ok","timestamp":1680101296522,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"L53HwaVY2ZO-","outputId":"2118aec8-0d1e-4770-98f2-bb070ce40ff2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RwKbYeTgbaTA"},"source":["## Importing dependencies and define helper functions"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2477,"status":"ok","timestamp":1680101298988,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"j6KpgCLGYWmj"},"outputs":[],"source":["\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import RecordVideo\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from typing import Any, List, Sequence, Tuple\n","from tensorflow.keras import layers\n","import tqdm\n","import statistics\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","import collections\n","\n","from tensorflow.keras.losses import Huber as huber\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ehbqP9CXbmo7"},"source":["## Tutorial: Loading CartPole environment"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"Go12dH4qbwBy"},"outputs":[],"source":["# CREATE ENV\n","env = gym.make(\"CartPole-v1\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9XZ9g3xrcAXE"},"source":["We can check the action and observation space of this environment. Discrete(2) means that there are two valid discrete actions: 0 & 1."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"ytxvVmLdcRyw","outputId":"3c28f6b8-7d5d-4582-af30-87af7f2b7d51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Discrete(2)\n"]}],"source":["\n","print(env.action_space)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pVXGWi_Ncfg-"},"source":["The observation space is given below. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"DyqHr9I5cdkX","outputId":"fa4f97bd-05a3-4b03-b638-1a7b309f28c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"]}],"source":["\n","print(env.observation_space)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HFOdaU2Gdyg0"},"source":["We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"VMr6qAqxdOsm","outputId":"7200cda8-5127-446b-ee0f-c986dee7e875"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial observations: [-0.01435321 -0.01604889  0.03553538  0.02224773]\n"]}],"source":["\n","observation = env.reset()\n","print(\"Initial observations:\", observation)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qnG2QdfbeZrI"},"source":["For the CartPole environment, there are two possible actions: 0 for pushing to the left and 1 for pushing to the right. For example, we can push the cart to the left using code below, which returns the new observation, the current reward, an indicator of whether the game ends, and some additional information (not used in this project). For CartPole, the game ends when the pole is significantly tilted or you manage to balance the pole for 500 steps. You get exactly 1 reward for each step before the game ends (i.e., max cumulative reward is 500)."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"MmfMDvyYdWGk","outputId":"39777ebd-3f4b-47d1-9355-5d590e0db514"},"outputs":[{"name":"stdout","output_type":"stream","text":["New observations after choosing action 0: [-0.01467418 -0.21166195  0.03598034  0.3259273 ]\n","Reward for this step: 1.0\n","Is this round done? False\n"]}],"source":["\n","observation, reward, done, info = env.step(0)\n","print(\"New observations after choosing action 0:\", observation)\n","print(\"Reward for this step:\", reward)\n","print(\"Is this round done?\", done)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tj0zCh59fhBb"},"source":["Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"AVucQVRwf6Jm","outputId":"6fcea59e-125a-43eb-eaee-b5bec8c53e50"},"outputs":[{"name":"stdout","output_type":"stream","text":["[-0.04537834 -0.19077793 -0.04205066  0.27981415] False {}\n","[-0.0491939  -0.38527557 -0.03645438  0.5589435 ] False {}\n","[-0.05689941 -0.57986736 -0.02527551  0.8399221 ] False {}\n","[-0.06849676 -0.7746353  -0.00847707  1.1245503 ] False {}\n","[-0.08398946 -0.9696451   0.01401394  1.4145623 ] False {}\n","[-0.10338236 -1.1649379   0.04230519  1.7115927 ] False {}\n","[-0.12668112 -1.3605193   0.07653704  2.0171363 ] False {}\n","[-0.1538915  -1.5563474   0.11687977  2.3324986 ] False {}\n","[-0.18501845 -1.7523164   0.16352974  2.6587307 ] False {}\n","[-0.22006477 -1.9482392   0.21670435  2.9965582 ] True {'TimeLimit.truncated': False}\n","Cumulative reward for this round: 10.0\n"]}],"source":["\n","observation = env.reset()\n","cumulative_reward = 0\n","done = False\n","while not done:\n","    observation, reward, done, info = env.step(0)\n","    print(observation, done, info)\n","    cumulative_reward += reward\n","print(\"Cumulative reward for this round:\", cumulative_reward)\n","# [cart position, velocity, pole angle, angular velocity]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dz_R4N8QZ5vM"},"source":["## Introduction"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5tm1P75GVS-G"},"source":["RL algorithms can be classified into critic-only, actor-only and actor-critic methods. The actor-critic method is used to combine the advantages of actor-only and critic-only methods. The parameterized actor has the benefit of being able to generate continuous actions without requiring optimization procedures on a value function, whereas the critic's advantage is providing the actor with low-variance information about its performance.\n","\n","Source: https://aadeshnpn.com/wp-content/uploads/2018/09/actor-critic-reinforcement.pdf\n","\n","The main method that we will be using is the Actor-Critic method. Actor-Critic methods utilize temporal difference (TD) learning techniques to model the policy function and value function separately. A value function determines the expected return for an agent following a certain policy indefinitely from a given state. On the other hand, a policy function provides a probability distribution across the possible actions depending on the present state.\n","\n","The Actor-Critic approach involves two components: the actor and the critic. The involved policy is the actor that will propose a set of potential actions based on the current situtation. For the other component, the critic, it is the estimated value function that will assess the actor's performance based on the current policy.\n","\n","The critic network establishes a correlation between the state vectors and the negative reinforcement value. Using TD learning, the network will gain knowledge of the expected value of a discounted sum of future negative reinforcement signals.\n","\n","Source: https://arxiv.org/pdf/1810.01940.pdf\n","\n","Source: https://ieeexplore.ieee.org/abstract/document/24809\n","\n","In the following sections, we propose an Actor-Critic method to help with the CartPole problem. It first starts off with the development of the Reinforcement Learning agent. One neural network with two outputs will be used to represent both the actor and critic. We will then demonstrate the effectiveness of the reinforcement learning agent, followed by rendering one episode that is played by the agent to visualize the mechanics behind the implementation."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2oIzK9SzhlWN"},"source":["## Task 1: Development of an RL agent"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Y3d7RQu4bhVS"},"source":["The following cell is where we define the neural network for the Actor-Critic method. The actor portion of the model will generate the probabilities for the possible actions, while the critic part of it will generate the critic value. We have created a class ActorCriticMain to define the model for use. By using a class, it provides the modularity that can help with troubleshooting and reuse of the model multiple times in the project. In is also a useful way of combining data and functionality. \n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"D2Of0efTb_Uh"},"outputs":[],"source":["\n","# ------------------------------------------------------------------------------\n","# ===Actor-Critic Network===\n","# Here, we used a single network for both the actor and the critic. We have two \n","# common lower layers and two outputs. For the output, one is for the actor, and \n","# the other is for the critic. The reason for using 1 neural network is that we \n","# do not have to train two different neural networks to understand the \n","# environment. We have created a class for the actor critic neural network \n","# as shown below. Our actor-critic model class will be derived from the base \n","# model class from the Tensorflow. The model will take in the state as the \n","# input and will output both action probabilities and critic value , which \n","# models the state-dependent value function. The goal is to train a model that  \n","# chooses actions based on a policy that maximizes expected return.\n","\n","\n","# ===About the Actor===\n","# The actor in this case will model the policy, which is the probability \n","# distribution. The actor layer will have as many outputs as there are actions, \n","# and the softmax activation is used as we are modeling probabilities, which \n","# need to sum to one. When selecting the actions, we are choosing from a \n","# discrete action space, and hence a categorical distribution will be used \n","# in the implementation. \n","# ------------------------------------------------------------------------------\n","\n","# Define Actor Critic Class\n","class ActorCriticMain(tf.keras.Model):\n","  # Define constructor and take the number of possible moves as input. \n","  def __init__(self, possible_moves, nodes_count_1, nodes_count_2):\n","    # Call super constructor\n","    super().__init__()\n","\n","    # --------------------------------------------------------------------------\n","    # ===Defining the Layers===\n","    # Here are fully connected Dense layers. We have two common layers and 2 \n","    # independent outputs. For keras, the number of input dimensions are \n","    # infererred, so we did not specify the input dimensions in the constructor. \n","    # The activation that we have used is relu\n","    # --------------------------------------------------------------------------\n","\n","    # We considered adding Dropout to avoid overfitting if the results are not \n","    # good but results seems satisfactory so we did not.\n","\n","    # We also tried a few number of hidden layers and found that by trial and \n","    # error, this is by far the optimal one.\n","    self.fully_connected_1 = layers.Dense(nodes_count_1, activation=\"relu\")\n","    self.fully_connected_2 = layers.Dense(nodes_count_2, activation=\"relu\")\n","\n","    # --------------------------------------------------------------------------\n","    # ===Layer Outputs===\n","    # We will have 2 separate outputs. The first output is the critic function. \n","    # It is single value with no activation. The actor is our second output, \n","    # which is also our policy pi. It will output the possible moves. Upon \n","    # getting this output, we will run it through a softmax activation. The \n","    # policy is just a probability distribution, so using the categorical \n","    # function in the later part, it will assign each probability to each \n","    # action. \n","    # --------------------------------------------------------------------------\n","\n","    # We have 2 output layers.\n","\n","    # Actor is policy pi which output the feasible action, 0 or 1 based on \n","    # probabilities using softmax function.\n","    self.actor = layers.Dense(possible_moves) \n","\n","    # Critics evaluate the action produced by the actor by computing the value \n","    # function. \n","    # Link: https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14\n","    self.critic = layers.Dense(1) \n","\n","\n","  # The call function is for the forward pass during the model training\n","  def call(self, inputs):\n","    # We will pass the information through the two common connected layers\n","    fully_connected_1_output = self.fully_connected_1(inputs)\n","    fully_connected_2_output = self.fully_connected_2(fully_connected_1_output)\n","    actor_tensor = self.actor(fully_connected_2_output)\n","    critic_tensor = self.critic(fully_connected_2_output)\n","\n","    # And return both the critic value and the actor policy pi\n","    return actor_tensor, critic_tensor\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"A_Lb9CkJhoe5"},"source":["### TrainingModel Class\n","\n","\n","In this class, the following steps were carried out to train the agent:  \n","1. Firstly, we get the training data for every episode using the agent placed in the environment.  \n","2. Secondly, determine the expected return for each time step.  \n","3. Next, generate the loss for the combined Actor-Critic method and update the network parameters with the newly computed gradient thereafter.  \n","4. Lastly, repeat the previous steps till the success criterion or maxiumum number of episodes are fulfilled"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680101298989,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"nbyknO3dOdpx"},"outputs":[],"source":["class TrainingModel:\n","  def __init__(self, value_of_gamma, maximum_moves_per_episode, possible_moves, \n","               nodes_count_1, nodes_count_2, optimizer, tf_huber):\n","    self.value_of_gamma = value_of_gamma\n","    self.maximum_moves_per_episode = maximum_moves_per_episode\n","    self.model = ActorCriticMain(possible_moves, nodes_count_1, nodes_count_2)\n","    self.optimizer = optimizer\n","    self.tf_huber = tf_huber\n","  \n","  # This function will return the observation, reward and done flag based on a given action\n","  def gym_step(self, action):\n","    '''\n","    At each step, \"gym_step\" method produces probabilities for actions and a \n","    value for the critic, both of which are determined by the current policy \n","    that is parameterized by the weights of the model.\n","    '''\n","    obs, reward, done, info = env.step(action)\n","    output_tuple = (obs.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n","    return output_tuple\n","\n","  # ----------------------------------------------------------------------------\n","  # ===Starting and running episode===\n","  # We need training data to train the actor-critic model, just like with \n","  # supervised learning. However, the model must be run in the environment in \n","  # order to acquire such data. Each episode generates training data, and the \n","  # model does a forward pass on the state of the environment at each time step \n","  # using its weights to generate action probabilities and the critic value \n","  # based on the current policy.\n","\n","  # For the function below, we get the learning data for each episode \n","  # for the model to run on. The \"start_eps\" method will sample the next action \n","  # from the action probabilities generated by the model, which would then \n","  # be applied to the environment to produce the next state and reward.\n","  # ----------------------------------------------------------------------------\n","\n","  # This function will run one episode to get training data\n","  def start_eps(self, initial_obs):\n","\n","    SIZE = 0\n","    obs = initial_obs\n","    initial_obs_shape = initial_obs.shape\n","    num_sequence_limit = tf.range(self.maximum_moves_per_episode)\n","\n","    # Here we use dynamic_size = True to allow TensorArray to be written past its initial size.\n","    rewards = tf.TensorArray(dtype=tf.int32, size = SIZE, dynamic_size=True)\n","    vals = tf.TensorArray(dtype=tf.float32, size = SIZE, dynamic_size=True)\n","    move_probs = tf.TensorArray(dtype=tf.float32, size = SIZE, dynamic_size=True)\n","\n","    # Here we run the model for maximum_moves times\n","    for curr in num_sequence_limit:\n","      # Here, the observation is converted into a batched tensor (batch size = 1). \n","      # We have to add an extra dimension (batched dim) as the deep neural network \n","      # expects a batch of inputs. \n","      obs = tf.expand_dims(obs, 0)\n","    \n","      # Call our action-critic model and generate the probabilities of actions \n","      # and critic value given the obs\n","      prob_dist, critic_v = self.model(obs)\n","    \n","      # Here, we draw next move (sample) from the categorical move probability \n","      # distribution. We use the probabilities defined by our neural network to \n","      # feed into the tensorflow probability categorical distribution and use \n","      # that to select a move by sampling the distribution. It will be needed \n","      # for the calculation for the log probability.\n","      move = tf.random.categorical(prob_dist, 1)[0, 0] # we put 1 to get 1 sample.\n","      move_probs_t = tf.nn.softmax(prob_dist) # generate the softmax activations\n","\n","      # Record critic values by plain writing to TensorFlow Array for vals.\n","      # We have to squeeze the value as the loss works best if it is on a one dimensional quantity.\n","      vals = vals.write(curr, tf.squeeze(critic_v))\n","\n","      # Record log probability of the chosen move by plain writing to TensorFlow Array for move_probs\n","      # Get the 0th element of that because we added in the batch dimension earlier for compatibility with our neural network.\n","      # This is the move that we saved at the top when we calculate the move for the agent. This is the most recent action.\n","      move_probs = move_probs.write(curr, move_probs_t[0, move])\n","    \n","      # Get next observation and reward based on the move taken\n","      # We convert gym_step, a python function to a tensorflow function to achieve this.\n","      obs, reward, done = tf.numpy_function(self.gym_step, [move], [tf.float32, tf.int32, tf.int32])\n","\n","      # Record reward by plain writing to TensorFlow Array for rewards\n","      rewards = rewards.write(curr, reward)\n","\n","      # Here, we update the shape of obs_shape tensor\n","      obs.set_shape(initial_obs_shape)\n","\n","      # We try casting done flag to boolean dtype. If true, it means \n","      if tf.cast(done, tf.bool):\n","        break\n","\n","    # We return all the values in the move_probs TensorArray as a stacked Tensor.\n","    move_probs = move_probs.stack()\n","\n","    # We return all the values in the rewards TensorArray as a stacked Tensor.\n","    rewards = rewards.stack()\n","\n","    # We return all the values in the vals TensorArray as a stacked Tensor.\n","    vals = vals.stack()\n","    \n","    return move_probs, vals, rewards\n","\n","\n","  # ----------------------------------------------------------------------------\n","  # ===Computing expected returns===\n","  # During a single episode, the rewards obtained at each time step are \n","  # transformed into a series of anticipated returns - whereby the rewards from \n","  # the current time step up to the maximum time step are summed up and \n","  # multiplying each reward by a progressively decreasing discount factor.\n","  # ----------------------------------------------------------------------------\n","\n","\n","  def generate_return(self, value_of_rewards, normalize = True):\n","    # Calculates the expected returns for every time step\n","\n","    # Here, we are setting the sizes of output first\n","    output = tf.TensorArray(dtype = tf.float32, size = tf.shape(value_of_rewards)[0])\n","\n","    # Begin with the final element of the rewards sequence and add up the rewards to store the resulting sums in the output array.\n","    decaying_reward = tf.constant(0.0)\n","    decaying_reward_shape = decaying_reward.shape\n","\n","    # We convert values_of_rewards to float values\n","    value_of_rewards = tf.cast(value_of_rewards[::-1], dtype = tf.float32)\n","    \n","    # We perform a for loop in tensorflow\n","    for curr in tf.range(tf.shape(value_of_rewards)[0]):\n","\n","      # Get the value of decaying_reward by decaying the reward (multiplying with gamma)\n","      decaying_reward = decaying_reward * self.value_of_gamma\n","\n","      # Adding curr reward to our decaying_reward\n","      decaying_reward += value_of_rewards[curr]\n","\n","      # Update the shape of decaying_reward tensor\n","      decaying_reward.set_shape(decaying_reward_shape)\n","\n","      # Write decaying_reward into curr index of 'output' TensorArray\n","      output = output.write(curr, decaying_reward)\n","\n","    # Here we return the values in the 'output' TensorArray as a stacked Tensor with 1 backward step.\n","    output = output.stack()[::-1]\n","\n","    if normalize:\n","      output = ((output - tf.math.reduce_mean(output)) / (tf.math.reduce_std(output) + np.finfo(np.float32).eps.item()))\n","\n","    return output\n","\n","\n","  # ----------------------------------------------------------------------------\n","  # ===Computing Actor-Critic Loss===\n","  # The actor-critic loss uses a loss function that combines the losses of both \n","  # the actor and the critic. The advantage, referred to as 'gain' in our code,\n","  # is contained in the actor loss formulation. The advantage is the degree of \n","  # improvement that can happen by taking the action in a specific state, \n","  # instead of choosing a random action that is based on the policy pi for that \n","  # state. If an action is doing better than expected, we want to prompt the \n","  # actor to replicate that action. Conversely, if an action is doing worse than \n","  # expected, the actor should be encouraged to take the opposite action. When \n","  # an action meets the expected performance level, there is no learning \n","  # opportunity for the actor. Hence, the advantage can decide which actions \n","  # to encourage by deciding how to scale the action that the agent just took. \n","  # Source: https://towardsdatascience.com/advantage-actor-critic-tutorial-mina2c-7a3249962fc8#:~:text=In%20the%20field%20of%20Reinforcement,input%20states%20to%20output%20actions.\n","\n","  # ===Actor Loss===\n","  # The Actor loss is determined using policy gradients, with the Critic \n","  # functioning as a baseline that is dependent on the current state. \n","  # Additionally, this loss is calculated using estimates derived from a single \n","  # sample per episode. \n","\n","  # ===Critic Loss===\n","  # For the critic loss, Huber loss is used for its calculation. Huber Loss is \n","  # a combination of both Mean Square Error and Mean Absolute Error. Mean Square\n","  # Error is good for learning the outliers, while Mean Absolute Error is good \n","  # for ignoring the outliers. Huber loss is good for situations where higher \n","  # weightage should not be given to the outliers. \n","  # Source: https://gobiviswa.medium.com/huber-error-loss-functions-3f2ac015cd45\n","  # ----------------------------------------------------------------------------\n","\n","\n","  # Calculates the combined loss for Actor-Critic \n","  def calculate_loss(self, move_probs,  values, return_vals):\n","\n","    # Calculate critic loss \n","    critic_loss = self.tf_huber(values, return_vals)\n","\n","    # Calculate actor loss \n","    # tf.math.log(move_probs) represents the log of move's probability\n","    # (return_vals - values) represents gain, which is how much better a move is based on a given state than a random move based on the policy pi\n","    actor_loss = -tf.math.reduce_sum(tf.math.log(move_probs) * (return_vals - values))\n","\n","    # Calculate combined loss \n","    combined_loss = critic_loss + actor_loss\n","\n","    return combined_loss\n","  \n","\n","  # ----------------------------------------------------------------------------\n","  # ===Training the Model===\n","  # This function, train() will be used to train the model. The \"train()\" \n","  # method will be run every episode. For a substantial speedup in training, \n","  # @tf.function context is added. To enable automatic differentiation for all \n","  # steps before computing the loss function, @tf.GradientTape() context is added. \n","\n","  # While the agent is exploring its surroundings, the critic network tries to \n","  # minimize the advantage function. Initially during the learning phase, the \n","  # critic will likely make large errors, causing the calculate temporal \n","  # difference error to be inaccurate. Since the critic lacks understanding of \n","  # the environment at the beginning, the actor cannot gain much knowledge from \n","  # the critic. However, as the critic begins to predict outcomes more accurately, \n","  # the advantage calculation improves in accuracy as well. Through the \n","  # repetition of this learning process over many game episodes, the actor and \n","  # critic will learn to balance the pole for a longer time.\n","  # Source: https://towardsdatascience.com/advantage-actor-critic-tutorial-mina2c-7a3249962fc8#:~:text=In%20the%20field%20of%20Reinforcement,input%20states%20to%20output%20actions.\n","  # ----------------------------------------------------------------------------\n","\n","  # Execute the model training the step\n","  @tf.function\n","  def train(self, initial_obs):\n","    # Calculate the actual gradient using gradient tape\n","    with tf.GradientTape() as tape:\n","\n","      # Consolidate the learning data by executing the model for 1 episode\n","      move_probs, values, rewards = self.start_eps(initial_obs) \n","\n","      # Here, we generate the expected returns\n","      returns = self.generate_return(rewards)\n","\n","      # Change the learning data to the corresponding Tensorflow tensor shapes\n","      move_probs, values, returns = [tf.expand_dims(x, 1) for x in [move_probs, values, returns]] \n","\n","      # The loss values will be calculated here\n","      loss_vals = self.calculate_loss(move_probs, values, returns)\n","\n","    # The calculated loss values will be used to update the neural network\n","    gradients = tape.gradient(loss_vals, self.model.trainable_variables)\n","\n","    # The gradients are applied to the neural network's parameters\n","    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n","\n","    episode_reward = tf.math.reduce_sum(rewards)\n","\n","    return episode_reward\n","\n","\n","# Code references: \n","# https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning\n","# https://keras.io/examples/rl/actor_critic_cartpole/"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1GaROBXqmSAo"},"source":["#### Run training loop\n","\n","We run the training step until either the success criterion has been met, or maximum number of episodes is reached.\n","\n","To summarize the overview of the algorithm, we will first initialise the actor-critic network. And by repeating for a large number of episodes, we will reset the environment, score and terminal flag. And while the state is not terminal, we will select the action according to the policy of the actor network. The action will be taken to receive the reward and the new state. The scores will be saved over time for evidence of learning.\n","\n","\n","From the results shown below, there is an overall increase in score over time. But we observed that there are oscillations in the scores. This can attributed to the fact that actor-critic methods are not very stable. This is also observed by Nadendra and his team (source: https://arxiv.org/pdf/1810.01940.pdf). Their team had also observed large oscillations when running their actor-critic policy gradient method. They postulated that the periodic variations could be due to the characteristics of actor-critic methods.\n","\n","\n","But the overall trend should be increasing. Another observation was that the score can go up for a while then fall off to a lower value. This can be attributed to the cause that the actor-critic method might not be the best solution for all the cases. But this can be set as the basis for more advanced algorithms that can be used for future work. "]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60148,"status":"ok","timestamp":1680101359127,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"XhXx0GsWmPnV","outputId":"b6a16363-a5c6-436c-eafe-5de59c0967c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 5/15000 [00:03<1:59:30,  2.09it/s] "]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 0 : 46\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 25/15000 [00:03<14:42, 16.96it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 20 : 40.61904761904762\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 50/15000 [00:04<05:59, 41.53it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 40 : 38.951219512195124\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 61/15000 [00:04<07:39, 32.52it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 60 : 41.24590163934426\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 88/15000 [00:05<06:57, 35.72it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 80 : 45.876543209876544\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 105/15000 [00:05<06:26, 38.53it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 100 : 44.02\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 122/15000 [00:06<08:17, 29.93it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 120 : 49.37\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 143/15000 [00:08<21:05, 11.74it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 140 : 72.51\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 165/15000 [00:09<09:22, 26.39it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 160 : 75.94\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 182/15000 [00:10<20:00, 12.34it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 180 : 88.88\n"]},{"name":"stderr","output_type":"stream","text":["  1%|▏         | 203/15000 [00:11<08:27, 29.17it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 200 : 93.06\n"]},{"name":"stderr","output_type":"stream","text":["  1%|▏         | 223/15000 [00:12<15:28, 15.91it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 220 : 94.91\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 241/15000 [00:15<51:22,  4.79it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 240 : 97.55\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 261/15000 [00:22<1:27:59,  2.79it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 260 : 169.43\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 282/15000 [00:26<26:16,  9.33it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 280 : 222.72\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 321/15000 [00:28<04:36, 53.16it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 300 : 236.6\n"," Average reward of episode 320 : 223.2\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 354/15000 [00:28<03:20, 73.01it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 340 : 192.65\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 371/15000 [00:28<03:35, 67.91it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 360 : 112.51\n"]},{"name":"stderr","output_type":"stream","text":["\r  3%|▎         | 379/15000 [00:29<08:55, 27.31it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 380 : 60.22\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 401/15000 [00:34<46:56,  5.18it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 400 : 118.1\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 420/15000 [00:40<1:05:44,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 420 : 211.97\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 441/15000 [00:44<48:46,  4.98it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 440 : 291.73\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 461/15000 [00:50<1:09:52,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 460 : 386.36\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 481/15000 [00:55<1:04:45,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":[" Average reward of episode 480 : 460.42\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 495/15000 [00:59<29:14,  8.27it/s]  "]},{"name":"stdout","output_type":"stream","text":["\n","\n","Finished training at episode 495 with an average reward of 477.16\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["\n","# Here we initialise our training model, actor-critic.\n","trainingAgent = TrainingModel(\n","    value_of_gamma = 0.99, \n","    maximum_moves_per_episode = 500, \n","    possible_moves = 2, \n","    nodes_count_1 = 256, \n","    nodes_count_2 = 128,\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n","    tf_huber = huber(reduction=tf.keras.losses.Reduction.SUM)\n","    # Instead of using MSE, we use Tensorflow Huber as it is less sensitive to outlier as mentioned in this link (It handles outliers better than): \n","    # https://www.numpyninja.com/post/loss-functions-when-to-use-which-one#:~:text=Huber%20Loss%20is%20often%20used,of%20both%20MSE%20and%20MAE.&text=a)%20Outliers%20are%20handled%20properly.\n","    # Here we initialise the tensor flow for huber loss for critic loss usage later.\n","    # We use SUM for loss reduction as we want the scalar sum of weighted loss for calculating critic loss later\n","    )\n","\n","maximum_episodes = 15000\n","minimum_episodes = 100\n","\n","# According to the link below, CartPole-v1 has a reward_threshold of 475. This means when the average rewards hit 475, it is considered solved. \n","# Hence, we set reward threshold as 475.\n","# https://stackoverflow.com/questions/56904270/difference-between-openai-gym-environments-cartpole-v0-and-cartpole-v1\n","curr_avg_reward = 0\n","reward_threshold = 475\n","\n","# Here, we will keep the reward of the latest 100 episode in a queue, which uses FIFO order, making it suitable for our use case.\n","episodes_reward = collections.deque(maxlen=minimum_episodes)\n","\n","bar_range = tqdm.trange(maximum_episodes)\n","\n","# Here we loop through all the episodes starting from 0 to at most 'maximum_episodes'.\n","for episode in bar_range:\n","    initial_obs = env.reset()\n","    initial_obs = tf.constant(initial_obs, dtype=tf.float32)\n","\n","    # Here, we train our model and get the eps_reward in return.\n","    eps_reward = int(trainingAgent.train(initial_obs))\n","    \n","    # We push the eps_reward into our queue, 'episodes_reward'\n","    episodes_reward.append(eps_reward)\n","\n","    # We generate the mean reward as of 'now'\n","    curr_avg_reward = statistics.mean(episodes_reward)\n","  \n","    # We print out the average episode reward for everytime 20 episodes is done.\n","    if episode % 20 == 0:\n","      print(' Average reward of episode', episode, \":\", curr_avg_reward)\n","    \n","    # We can only stop training if it has run at least 100 episodes and when our average reward has met the expectated reward_threshold.\n","    if episode >= minimum_episodes:\n","      if curr_avg_reward > reward_threshold:\n","        break\n","\n","print('\\n')\n","\n","# Here we print out the episode it stops training at and the average reward at that point in time.\n","print('Finished training at episode', episode, 'with an average reward of', curr_avg_reward)\n","\n","\n","# Code reference: Self implemented"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3491,"status":"ok","timestamp":1680101362605,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"wDFxqEknm8bb"},"outputs":[],"source":["# SAVE MODEL\n","# Here, we save our model for future usage. Change the dir accordingly.\n","# dir should be pointing towards a folder for storing the model\n","dir = \"/content/drive/MyDrive/CZ3005 AI Project/Project 1/JOVAN_BRYAN_GLENN_A33/model\"\n","trainingAgent.model.save(dir, save_format=\"tf\")\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0mu9pzh4qVAv"},"source":["### Checkpoint to skip training and test the model saved on local directory"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1680101362605,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"GF8UY38Bm9uF","outputId":"e9f74b6b-d5fb-4c71-e506-9d0a2ea38375"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["# CHECKPOINT SKIP TRAINING\n","# Load model. Change dir accordingly.\n","# dir should be pointing towards the folder of the saved model in our submission.\n","dir = \"/content/drive/MyDrive/CZ3005 AI Project/Project 1/JOVAN_BRYAN_GLENN_A33/model\"\n","loaded_model = tf.keras.models.load_model(dir)\n","\n","# Code reference: Self implemented"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680101362605,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"t27i_b1Hn5NO"},"outputs":[],"source":["\n","# Here, we create a function that return the model's output (action) based on an input (observation)\n","def actor_critic_agent(observation):\n","  observation = tf.constant(observation, dtype=tf.float32)\n","  observation = tf.expand_dims(observation, 0)\n","  action_probs, _ = loaded_model(observation)\n","  action = np.argmax(np.squeeze(action_probs))\n","  return action\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RAi7KKwNiegR"},"source":["For Task 1, we can show the observation and chosen action below:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680101362605,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"ae2ia-vUiNKJ","outputId":"2c443038-a377-4a39-f381-9b359c51bd63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Observation: [ 0.00680826 -0.04363387  0.0197934  -0.01903799]\n","Chosen action: 1\n"]}],"source":["\n","observation = env.reset()\n","action = actor_critic_agent(observation)\n","print(\"Observation:\", observation)\n","print(\"Chosen action:\", action)\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-XtIQ0Rti1gm"},"source":["## Task 2: Demonstrate the effectiveness of the RL agent"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"djBEShf0kGI4"},"source":["For this task, use the agent developed in Task 1 to play the game (refer to tutorial for how to play a round), record the cumulative reward for each round, and plot the reward for each round. \n","\n","We have implemented this section to fulfil task 2, which is to demonstrate the effectiveness of the RL agent We have run it for a total of 100 episodes, and for each new episode, we will reset the environment. Overall, our reinforcement learning algorithm uses an actor-critic agent to play the game for 100 episodes. "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62531,"status":"ok","timestamp":1680101425117,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"qgCIkqvZXFee","outputId":"1ec92e45-e4da-47b2-a22e-0c089f2a0a44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","Episode  finished  at  t500\n","[500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n"]}],"source":["\n","# Here, we perform task 2 to test how effective our RL agent (actor-critic model) is\n","n_episodes = 100 # assignment's requirement\n","episode_results = [] # we store the reward results for every episodes for plotting later\n","\n","# We run for 100 episodes as part of assignment's requirement\n","for episode in range(n_episodes):\n","  cumulative_reward = 0\n","  observation = env.reset()\n","  for t in range(500): # each episode will have 500 steps as part of assignment's requirement\n","    action = actor_critic_agent(observation) # function called to get model's output (action) based on input (observation)\n","    observation, reward, done, info = env.step(action) # next step\n","    cumulative_reward += reward # get cumulative reward\n","    if done: # if done flag returns True, it means episode has ended (either it has finished 500 steps and truncated or it was terminated)\n","      print(\"Episode  finished  at  t{}\".format(t+1))\n","      episode_results.append(cumulative_reward)\n","      break\n","print(episode_results)\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"J_0R5a4RhA_z"},"source":["Here, we used the numpy and matplotlib libraries to plot the cumulative reward for each episode while running the model that we have exported. The results are first converted to a numpy array so that it can be used for the graph visualisation. The output will be a graph that plots the cumulative reward for each episode, with the input as a list of episode results for our reinforcement learning algorithm. \n","\n","From the plotted graph, it can bee seen that the cumulative reward remains at 500 for every of the 100 episodes. The values are consistently more than 195 for all of the trials. The average cumulative reward was also computed, and it is more than the requirement of 195. This suggests that the agent has learned to perform the task and has demonstrated good enough results. "]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1680101425118,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"RZrCKywQi6CE","outputId":"6131c613-ced1-482f-8cd9-6c0c62bc5818"},"outputs":[{"name":"stdout","output_type":"stream","text":["[500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500. 500.\n"," 500. 500.]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","\n","# Here we convert list to numpy for plotting convenience.\n","episode_results = np.array(episode_results)\n","print(episode_results)\n","plt.plot(episode_results)\n","plt.title('Cumulative reward for each episode')\n","plt.ylabel('Cumulative reward')\n","plt.xlabel('episode')\n","plt.show()\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XndSYH7wlvn7"},"source":["Print the average reward over the 100 episodes."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1680101425118,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"pOiOp9OYlo5Y","outputId":"ae407511-e159-4283-f861-7860bb43888c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average cumulative reward: 500.0\n","Is my agent good enough? True\n"]}],"source":["\n","# Here we check if our agent passed assignment's requirement and if it is good enough.\n","print(\"Average cumulative reward:\", episode_results.mean())\n","print(\"Is my agent good enough?\", episode_results.mean() > 195)\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Yg0DCT38lFA6"},"source":["## Task 3: Render one episode played by the agent"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vx1awMr9lc_w"},"source":["Here, our agent is plugged in to demonstrate the rendered result.\n","\n","If you cannot see our video rendered here, look at the submission folder for a video called 'one_episode_played_by_agent.mp4'.  \n","You can also head over to our google colab workspace to see the video rendered there.  \n","Feel free to duplicate it if you wish to run on google colab.  \n","Google Colab: https://drive.google.com/file/d/1dgJvCFymMeUToSsbR4JSetgoZSnAClm1/view?usp=sharing"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":3917,"status":"ok","timestamp":1680101429020,"user":{"displayName":"Jovan Huang","userId":"11892135888426203809"},"user_tz":-480},"id":"UMUaMukboXD-","outputId":"145540f8-1523-42b4-9d5d-6ff1cfa8f15d"},"outputs":[{"data":{"text/html":["<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","             </video>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","# Here we run one episode to see a video played by the agent\n","env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n","observation = env.reset()\n","while True:\n","    env.render()\n","    #your agent goes here\n","    action = actor_critic_agent(observation) # function called to get model's output (action) based on input (observation)\n","    observation, reward, done, info = env.step(action) # next step\n","    if done: # if done flag returns True, it means episode has ended (either it has finished 500 steps and truncated or it was terminated)\n","      break;    \n","env.close()\n","show_video()\n","\n","\n","# Code reference: Self implemented"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rCY7wtmcfEUW"},"source":["# Conclusion\n","\n","In conclusion, we have managed to implement a reinforcement learning method to solve the problem. This is by done by creating an actor-critic neural network and training in the environment. We have also demonstrated the effectiveness of the RL agent and rendered an animation to demonstrate the movements of the algorithm. Our choice for the use of actor-critic is due to the advantages that it brings: The actor-critic method requires minimal computation for seleting actions and they are also able to learn an explicitly stochastic policy by learning the optimal probabilities of selecting various actions. \n","\n","Source: http://www.incompleteideas.net/book/ebook/node66.html#:~:text=The%20policy%20structure%20is%20known,being%20followed%20by%20the%20actor."]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
